{
  "filename": "index.md",
  "document_type": "md",
  "total_chunks": 25,
  "total_pages": 1,
  "loading_method": "basic",
  "loading_strategy": null,
  "chunking_strategy": "basic",
  "chunking_method": "loaded",
  "timestamp": "2025-05-05T12:32:22.892358",
  "chunks": [
    {
      "content": "对神经网络架构的思考",
      "metadata": {
        "chunk_id": 1,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "category_depth": 0,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "element_type": "Title",
        "id": "58598a0dab840a72e43319a6481a9e04",
        "category": "Title",
        "chunk_index": 1
      }
    },
    {
      "content": "鱼XueTr 鱼XueTr AI开发/Web全栈/Rust/Node.js/Python 2 人赞同了该文章",
      "metadata": {
        "chunk_id": 2,
        "page_number": 1,
        "page_range": "1",
        "word_count": 5,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "58598a0dab840a72e43319a6481a9e04",
        "element_type": "Text",
        "id": "a792a27d72684254f070b4d92fa909a5",
        "category": "UncategorizedText",
        "chunk_index": 2
      }
    },
    {
      "content": "自从2018年开始接触深度学习网络到现在，已经过去了7年时间，从最开始接触的CNN，到后来的RNN，LSTM，GRU， 再到后来的Transformer，以及最近大火的LLM，在接触这些网络架构的时候， 我一直在思考一个问题，这些网络架构到底是如何设计出来的，以及这些网络架构的原理是什么。",
      "metadata": {
        "chunk_id": 3,
        "page_number": 1,
        "page_range": "1",
        "word_count": 3,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "58598a0dab840a72e43319a6481a9e04",
        "element_type": "Text",
        "id": "746a1d28354666d83de2a954ac0d25b6",
        "category": "UncategorizedText",
        "chunk_index": 3
      }
    },
    {
      "content": "神经网络的零件",
      "metadata": {
        "chunk_id": 4,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "category_depth": 1,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "58598a0dab840a72e43319a6481a9e04",
        "element_type": "Title",
        "id": "5487b990326b3cb78d24a70729eb26ad",
        "category": "Title",
        "chunk_index": 4
      }
    },
    {
      "content": "当然每个阶段思考的问题是不一样的，在最初的时候是拼命的想要理解网络的原理，为什么能获得预期的效果， 最开始接触的是CNN,当时不论上班还是下班，都在看相关的论文，看相关的视频，看相关的博客，当时资料比较少， 书一遍一遍翻，思考每一个参数的作用和意义，比如卷积层的步长、卷积的方式等等，计算输入图像的大小怎么才能算出准确的输出等。 现在看来当时思考的问题就是要想明白一个网络的单独零件的作用和含义的。",
      "metadata": {
        "chunk_id": 5,
        "page_number": 1,
        "page_range": "1",
        "word_count": 4,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "5487b990326b3cb78d24a70729eb26ad",
        "element_type": "Text",
        "id": "e7fefbb86dee24f58fa8f30b45236644",
        "category": "UncategorizedText",
        "chunk_index": 5
      }
    },
    {
      "content": "比如卷积如何提取特征，池化如何做特征筛选，线性层如何计算特征和转换特征，激活函数如何增加非线性。",
      "metadata": {
        "chunk_id": 6,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "5487b990326b3cb78d24a70729eb26ad",
        "element_type": "Text",
        "id": "10e975a8208acfb9d2a3f640ae5bee3a",
        "category": "UncategorizedText",
        "chunk_index": 6
      }
    },
    {
      "content": "当时看到卷积的操作，有一次突然联想到数学期望，突然感觉卷积的加权求和形式上不就和高中学的数学期望很像吗， 数学期望的本身也是计算一个预测值，而卷积操作不也是加权求和吗，后来想单一的这一步骤其实用数学期望解释好像也蛮有道理的。 当然这个想法也就自己想想，毕竟不是什么权威。在数学中都是符合和形式表示的，在形式上或者含义上说得通，但可能定义域和值域不一样。 形式上相同也不完全等价，当时就觉得这样理解也可以，只给一两个朋友这么说过，毕竟只是自己的理解而已。",
      "metadata": {
        "chunk_id": 7,
        "page_number": 1,
        "page_range": "1",
        "word_count": 4,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "5487b990326b3cb78d24a70729eb26ad",
        "element_type": "Text",
        "id": "b422d8b85e27a2245ffdb4d7d02038ec",
        "category": "UncategorizedText",
        "chunk_index": 7
      }
    },
    {
      "content": "不光CNN的组件是提取、丢弃、计算或转换的基本思想。其实RNN也是类似的思路，比如LSTM的输入门、输出门、遗忘门， 其实也是对信息的一种筛选，只不过RNN的筛选是基于时间序列的，而CNN的筛选是基于空间序列的。",
      "metadata": {
        "chunk_id": 8,
        "page_number": 1,
        "page_range": "1",
        "word_count": 2,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "5487b990326b3cb78d24a70729eb26ad",
        "element_type": "Text",
        "id": "c9090d994bbf356bf5ab7002f9c8f81e",
        "category": "UncategorizedText",
        "chunk_index": 8
      }
    },
    {
      "content": "神经网络的结构",
      "metadata": {
        "chunk_id": 9,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "category_depth": 1,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "58598a0dab840a72e43319a6481a9e04",
        "element_type": "Title",
        "id": "fdb50eaef58d078d1e1c56d7ce77ea54",
        "category": "Title",
        "chunk_index": 9
      }
    },
    {
      "content": "理解了神经网络的零件，接下来就是理解神经网络的结构，最早接触的也是CNN网络，就在脑海中把CNN的结构画出来， 卷积层提取特征（加权求和），池化层做特征筛选（舍小区大），全连接层做特征转换（矩阵乘法的意义），激活函数增加非线性（非线性曲线）。 连接起来后就是：提取->丢弃->转换->非线性->提取->丢弃->转换->非线性->...->输出。 本身有90000(3003001的图像)个特征经过逐层提取，丢弃，转换，非线性，最后变成一个只有1024个特征的向量。 感觉就是一个逐层选拔的过程，最后留下来的都是精华。跟现实生活中的选拔过程很像，最后留下来的都是精英。 留下来的精华特征(1024)代表了输入的整张图像(90000)，就像一场竞赛一样，最终留下来的冠军、亚军等，代表了这一届人才的能力。 最终留下来的1024特征就这张图像(90000)的代表，可以代表整张图像。",
      "metadata": {
        "chunk_id": 10,
        "page_number": 1,
        "page_range": "1",
        "word_count": 7,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "fdb50eaef58d078d1e1c56d7ce77ea54",
        "element_type": "Text",
        "id": "1b079a2b8da05f63c1bed48e0281141c",
        "category": "UncategorizedText",
        "chunk_index": 10
      }
    },
    {
      "content": "LSTM的每个组件都相同，它们的筛选和丢弃，是对之前(上一步)的信息进行筛选，然后丢弃一些信息，也就是对历史信息进行筛选的过程。",
      "metadata": {
        "chunk_id": 11,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "fdb50eaef58d078d1e1c56d7ce77ea54",
        "element_type": "Text",
        "id": "a507af008ccd23f7c2118679da11e8d5",
        "category": "UncategorizedText",
        "chunk_index": 11
      }
    },
    {
      "content": "Transformer也是处理序列结构的，在某种意义上可以认为Transformer是LSTM的并行版本，以并行代替循环。Transformer是基于自注意力机制一次性 把所有时间步的特征都计算出来，然后也是对信息的提取和筛选。",
      "metadata": {
        "chunk_id": 12,
        "page_number": 1,
        "page_range": "1",
        "word_count": 2,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "fdb50eaef58d078d1e1c56d7ce77ea54",
        "element_type": "Text",
        "id": "2c2eb2fba731ce0ef9ba4dcd266a6d1e",
        "category": "UncategorizedText",
        "chunk_index": 12
      }
    },
    {
      "content": "神经网络的架构 再到后来，Transformer的架构被广泛应用，比如BERT，GPT，T5等，这些网络的架构都是基于Transformer的， 其实这里出现的新东西不如说是GPT，之前出现的大多网络其实算是编码器，本质上是对信息的筛选和汇总的作用， 而GPT是基于解码器的结构，也就是生成式的架构。",
      "metadata": {
        "chunk_id": 13,
        "page_number": 1,
        "page_range": "1",
        "word_count": 4,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "fdb50eaef58d078d1e1c56d7ce77ea54",
        "element_type": "Text",
        "id": "bedcc032ea9b132cb6e357e8b2e33edb",
        "category": "UncategorizedText",
        "chunk_index": 13
      }
    },
    {
      "content": "当前之前也有生成式的架构，比如GAN，基于生成式和判别式，但这种模式本质上其实是两个不同角色的编码器。 GPT网络的诞生，网络的架构从编码器转成解码器。",
      "metadata": {
        "chunk_id": 14,
        "page_number": 1,
        "page_range": "1",
        "word_count": 2,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "fdb50eaef58d078d1e1c56d7ce77ea54",
        "element_type": "Text",
        "id": "70dec93463650b0bb5b81ade25771799",
        "category": "UncategorizedText",
        "chunk_index": 14
      }
    },
    {
      "content": "解码器的本质其实很类似考古，根据出现的信息，推测之前可能发生的事情或者发生过的场景。如果你愿意，就当做是拿到一块化石， 然后根据这块化石推测这块化石之前可能发生的事情或者发生过的场景。",
      "metadata": {
        "chunk_id": 15,
        "page_number": 1,
        "page_range": "1",
        "word_count": 2,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "fdb50eaef58d078d1e1c56d7ce77ea54",
        "element_type": "Text",
        "id": "2c13cd113749661629ba1f7ec9194ca9",
        "category": "UncategorizedText",
        "chunk_index": 15
      }
    },
    {
      "content": "除了大多数的以编码为主的网络，还有以解码为主的GPT。还有基于编码器-解码器架构。 比较早的编码器-解码器架构，比如Seq2Seq，其实也是编码器和解码器，主要应用于机器翻译， 还有UNet，也是编码器-解码器架构，主要应用于图像分割。 再后来出现的Stable Diffusion，也是基于编码器-解码器架构，主要应用于图像生成,它的本质是将一张有噪声的图像 通过编码器-解码器架构，生成一张清晰度更高的图像，跟分割很像，也是剥离出来图像的一部分。",
      "metadata": {
        "chunk_id": 16,
        "page_number": 1,
        "page_range": "1",
        "word_count": 6,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "fdb50eaef58d078d1e1c56d7ce77ea54",
        "element_type": "Text",
        "id": "47b593020d55638fa4e9e8789242832f",
        "category": "UncategorizedText",
        "chunk_index": 16
      }
    },
    {
      "content": "总结",
      "metadata": {
        "chunk_id": 17,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "category_depth": 1,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "58598a0dab840a72e43319a6481a9e04",
        "element_type": "Title",
        "id": "07d1bb37119f94e521fa291f08db158a",
        "category": "Title",
        "chunk_index": 17
      }
    },
    {
      "content": "出现过大大小小很多的网络，大体上可以分为:编码器架构，解码器架构，编码器-解码器架构。 编码器很像是溪流归海，对信息的汇总；解码器很像是考古，根据出现的信息，推测之前可能发生的事情或者发生过的场景。 编码器-解码器架构，很像是编码器和解码器之间的对话，编码器提出问题，解码器回答问题，编码器-解码器对信息模式转换(比如翻译) 或者模态转换(比如文生图，图生文等)。",
      "metadata": {
        "chunk_id": 18,
        "page_number": 1,
        "page_range": "1",
        "word_count": 4,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "07d1bb37119f94e521fa291f08db158a",
        "element_type": "Text",
        "id": "b0ac1c809b618711055acb098df1bee2",
        "category": "UncategorizedText",
        "chunk_index": 18
      }
    },
    {
      "content": "以上是从一种形而上的层面形象化的理解，便于自己从不同层面来思考神经网络的意义所在。 从算子到网络结构，再到网络架构，神经网络都承担着不用的功能。",
      "metadata": {
        "chunk_id": 19,
        "page_number": 1,
        "page_range": "1",
        "word_count": 2,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "07d1bb37119f94e521fa291f08db158a",
        "element_type": "Text",
        "id": "0f4112687394a6f3a427b52271ce3537",
        "category": "UncategorizedText",
        "chunk_index": 19
      }
    },
    {
      "content": "都是自己浅薄的认知或者说是对自己学过的神经网络的一些胡思乱想，不一定对，不过不怕错，如果想错了总会随着认知拨乱反正的。",
      "metadata": {
        "chunk_id": 20,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "07d1bb37119f94e521fa291f08db158a",
        "element_type": "Text",
        "id": "e43bc177d4bb2b14ec0a15ff6ce2d0b4",
        "category": "UncategorizedText",
        "chunk_index": 20
      }
    },
    {
      "content": "欢迎讨论。",
      "metadata": {
        "chunk_id": 21,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "07d1bb37119f94e521fa291f08db158a",
        "element_type": "Text",
        "id": "1677e90acb41c9c90a3ef9d99e9b94ed",
        "category": "UncategorizedText",
        "chunk_index": 21
      }
    },
    {
      "content": "补充",
      "metadata": {
        "chunk_id": 22,
        "page_number": 1,
        "page_range": "1",
        "word_count": 1,
        "category_depth": 1,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "58598a0dab840a72e43319a6481a9e04",
        "element_type": "Title",
        "id": "a621ce7d36815b3558ecb4f6a10c82d1",
        "category": "Title",
        "chunk_index": 22
      }
    },
    {
      "content": "卷积与数学期望的类比 虽然卷积操作和数学期望在形式上都涉及加权求和，但两者的定义域、值域和应用场景不同。 卷积通常用于提取局部特征，而数学期望是概率论中的概念。因此，将两者等同可能导致误解。 LSTM 的信息筛选机制 文章将LSTM的输入门、遗忘门和输出门描述为对信息的筛选和丢弃，这种描述虽然形象，但略显简化。 实际上，LSTM通过门控机制控制信息的保留和更新，以捕捉长期依赖关系。",
      "metadata": {
        "chunk_id": 23,
        "page_number": 1,
        "page_range": "1",
        "word_count": 7,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "a621ce7d36815b3558ecb4f6a10c82d1",
        "element_type": "Text",
        "id": "3dbd1fe854d68822702eae119a10fa06",
        "category": "UncategorizedText",
        "chunk_index": 23
      }
    },
    {
      "content": "Transformer 与 LSTM 的关系 将Transformer描述为LSTM的并行版本可能引起误解。 虽然两者都处理序列数据，但Transformer完全摒弃了循环结构，依赖自注意力机制实现并行处理。因此，二者在架构和工作原理上有本质区别。 GPT 与考古的类比 将GPT的生成过程比作考古学家的推测，虽然形象，但可能导致误解。 GPT通过学习大量文本数据，基于已有上下文预测下一个词，属于自回归生成模型。这种过程更类似于语言模型的预测，而非对过去事件的推测。",
      "metadata": {
        "chunk_id": 24,
        "page_number": 1,
        "page_range": "1",
        "word_count": 10,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "a621ce7d36815b3558ecb4f6a10c82d1",
        "element_type": "Text",
        "id": "3c149b8ea71ec4cb061479022bfe02d4",
        "category": "UncategorizedText",
        "chunk_index": 24
      }
    },
    {
      "content": "Stable Diffusion 的工作原理 文章将Stable Diffusion描述为将有噪声的图像通过编码器-解码器架构生成清晰图像，这种描述略显简化。 实际上，Stable Diffusion是一种基于扩散过程的生成模型，通过逐步去噪生成图像，编码器-解码器结构在其中起到特征提取和重建的作用",
      "metadata": {
        "chunk_id": 25,
        "page_number": 1,
        "page_range": "1",
        "word_count": 7,
        "languages": [
          "zho"
        ],
        "file_directory": "temp",
        "filename": "index.md",
        "filetype": "text/markdown",
        "last_modified": "2025-05-05T12:32:22",
        "parent_id": "a621ce7d36815b3558ecb4f6a10c82d1",
        "element_type": "Text",
        "id": "430b7c59f5adea81bd5c1caa669d57cf",
        "category": "UncategorizedText",
        "chunk_index": 25
      }
    }
  ],
  "encoding": "utf-8"
}
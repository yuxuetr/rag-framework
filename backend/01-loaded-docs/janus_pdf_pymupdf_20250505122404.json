{
  "filename": "janus_pro_tech_report.pdf",
  "document_type": "pdf",
  "total_chunks": 13,
  "total_pages": 13,
  "loading_method": "pymupdf",
  "loading_strategy": null,
  "chunking_strategy": null,
  "chunking_method": "loaded",
  "timestamp": "2025-05-05T12:24:04.302179",
  "chunks": [
    {
      "content": "Janus-Pro: Unified Multimodal Understanding and\nGeneration with Data and Model Scaling\nXiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan\nDeepSeek-AI\nProject Page: https://github.com/deepseek-ai/Janus\nAbstract\nIn this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specif-\nically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data,\nand (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant\nadvancements in both multimodal understanding and text-to-image instruction-following capa-\nbilities, while also enhancing the stability of text-to-image generation. We hope this work will\ninspire further exploration in the field. Code and models are publicly available.\n1. Introduction\n0\n2\n4\n6\n8\n10\n12\nLLM Parameters (Billions)\n46\n52\n58\n64\nAverage Performance\nJanus-Pro-1B\nJanus-Pro-7B\nJanus\nLLaVA-v1.5-Phi-1.5*\nLLaVA-v1.5-7B*\nEmu3-Chat\nShow-o-256\nShow-o-512\nTokenFlow-XL\nVILA-U\nJanus-Pro Family (Unified Model)\nLLaVA Family (Understanding Only)\nShow-o Family (Unified Model)\n(a) Average performance on four multimodal understand-\ning benchmarks.\nGenEval\nDPG-Bench\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n55.0\n74.7\n43.0\n63.2\n48.0\n71.1\n67.0\n83.5\n74.0\n84.1\n54.0\n80.6\n61.0\n79.7\n80.0\n84.2\nSDXL\nSDv1.5\nPixArt-\nDALL-E 3\nSD3-Medium\nEmu3-Gen\nJanus\nJanus-Pro-7B\n(b) Performance on instruction-following benchmarks for\ntext-to-image generation.\nFigure 1 | Multimodal understanding and visual generation results from our Janus-Pro. For multi-\nmodal understand, we average the accuracy of POPE, MME-Perception, GQA, and MMMU. The scores of\nMME-Perception are divided by 20 to scale to [0, 100]. For visual generation, we evaluate the performance\non two instruction-following benchamrks, GenEval and DPG-Bench. Overall, Janus-Pro outperforms the\nprevious state-of-the-art unified multimodal models as well as some task-specific models. Best viewed\non screen.",
      "metadata": {
        "chunk_id": 1,
        "page_number": 1,
        "page_range": "1",
        "word_count": 271
      }
    },
    {
      "content": "The face of a beautiful girl.\nJanus-Pro-7B\nJanus\nÁü≠ prompt ÔºåÁæéÊÑüÔºåÁªÜËäÇ\nA clear image of a blackboard with a clean,\ndark green surface and the word 'Hello' written\nprecisely and legibly in the center with bold,\nwhite chalk letters.\nJanus-Pro-7B\nJanus\nCapture a close-up shot of a vibrant sunflower\nin full bloom, with a honeybee perched on its\npetals, its delicate wings catching the sunlight.\nJanus-Pro-7B\nJanus\nA minimalist photo of an orange tangerine\nwith a green stem and leaves, symbolizing\nprosperity, sitting on a red silk cloth during\nChinese New Year.\nA steaming cup of coffee on a wooden table.\nA glass of red wine on a reflective surface.\nFigure 2 | Comparison of text-to-image generation between Janus-Pro and its predecessor,\nJanus. Janus-Pro delivers more stable outputs for short prompts, with improved visual quality,\nricher details, and the ability to generate simple text. The image resolution is 384 √ó 384. Best\nviewed on screen.\nRecent advancements in unified multimodal understanding and generation models have\ndemonstrated significant progress [30, 40, 45, 46, 48, 50, 54, 55]. These approaches have been\nproven to enhance the instruction-following capabilities in visual generation tasks while re-\nducing model redundancy. Most of these methods utilize the same visual encoder to process\ninputs for both multimodal understanding and generation tasks. Since the representations\nrequired for these two tasks differ, this often results in suboptimal performance in multimodal\nunderstanding. To address this issue, Janus [46] proposes decoupling visual encoding, which\nalleviates the conflict between multimodal understanding and generation tasks, achieving\nexcellent performance in both tasks.\nAs a pioneering model, Janus is validated at the 1B parameter scale. However, due to the\nlimited amount of training data and the relatively small model capacity, it exhibites certain\nshortcomings, such as suboptimal performance on short prompts image generation and unstable\ntext-to-image generation quality. In this paper, we introduce Janus-Pro, an enhanced version of\nJanus that incorporates improvements across three dimensions: training strategies, data, and\nmodel size. The Janus-Pro series includes two model sizes: 1B and 7B, demonstrating scalability\nof the visual encoding decoding method.\nWe evaluate Janus-Pro on multiple benchmarks, and the results reveal its superior multi-\nmodal understanding capabilities and significantly improved text-to-image instruction-following\nperformance. Specifically, Janus-Pro-7B achieved a score of 79.2 on the multimodal understand-\ning benchmark MMBench [29], surpassing state-of-the-art unified multimodal models such as\nJanus [46] (69.4), TokenFlow [34] (68.9) and MetaMorph [42] (75.2). Additionally, in the text-to-\nimage instruction-following leaderboard GenEval [14], Janus-Pro-7B scores 0.80, outperforming\nJanus [46] (0.61), DALL-E 3 (0.67), and Stable Diffusion 3 Medium [11] (0.74).\n2",
      "metadata": {
        "chunk_id": 2,
        "page_number": 2,
        "page_range": "2",
        "word_count": 425
      }
    },
    {
      "content": "Auto-Regressive Transformer\nUnd. Encoder\nText Tokenizer\nGen. Encoder\nText De-Tokenizer\nText Tokenizer\nImage Decoder\nImage: X!\nLanguage Instruct: X\"\nLanguage Instruct: X\"\nImage: X!\nLanguage Response: X#\nGenerated Image: X!\nUnderstanding\nImage Generation\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶\nFigure 3 | Architecture of our Janus-Pro. We decouple visual encoding for multimodal under-\nstanding and visual generation. ‚ÄúUnd. Encoder‚Äù and ‚ÄúGen. Encoder‚Äù are abbreviations for\n‚ÄúUnderstanding Encoder‚Äù and ‚ÄúGeneration Encoder‚Äù, respectively. Best viewed on screen.\n2. Method\n2.1. Architecture\nThe architecture of Janus-Pro is shown in Figure 3, which is the same as Janus [46]. The core\ndesign principle of the overall architecture is to decouple visual encoding for multimodal\nunderstanding and generation. We apply independent encoding methods to convert the raw\ninputs into features, which are then processed by an unified autoregressive transformer. For\nmultimodal understanding, we use the SigLIP [53] encoder to extract high-dimensional semantic\nfeatures from images. These features are flattened from a 2-D grid into a 1-D sequence, and an\nunderstanding adaptor is used to map these image features into the input space of the LLM. For\nvisual generation tasks, we use the VQ tokenizer from [38] to convert images into discrete IDs.\nAfter the ID sequence is flattened into 1-D, we use a generation adaptor to map the codebook\nembeddings corresponding to each ID into the input space of the LLM. We then concatenate\nthese feature sequences to form a multimodal feature sequence, which is subsequently fed into\nthe LLM for processing. Apart from the built-in prediction head in the LLM, we also utilize a\nrandomly initialized prediction head for image predictions in the visual generation task. The\nentire model adheres to an autoregressive framework.\n2.2. Optimized Training Strategy\nThe previous version of Janus employs a three-stage training process. Stage I focuses on\ntraining the adaptors and the image head. Stage II handles unified pretraining, during which all\ncomponents except the understanding encoder and the generation encoder has their parameters\nupdated. Stage III is supervised fine-tuning, building upon Stage II by further unlocking the\nparameters of the understanding encoder during training. This training strategy has certain\nissues. In Stage II, Janus divides the training for text-to-image capabilities into two parts\nfollowing PixArt [4]. The first part trains on ImageNet [9] data, using image category names\nas prompts for text-to-image generation, with the goal of modeling pixel dependence. The\nsecond part trains on normal text-to-image data. During implementation, 66.67% of the text-\nto-image training steps in Stage II are allocated to the first part. However, through further\n3",
      "metadata": {
        "chunk_id": 3,
        "page_number": 3,
        "page_range": "3",
        "word_count": 418
      }
    },
    {
      "content": "experimentation, we find that this strategy is suboptimal and lead to significant computational\ninefficiency.\nTo address this issue, we make two modifications.\n‚Ä¢ Longer Training in Stage I: We increase the training steps in Stage I, allowing sufficient\ntraining on the ImageNet dataset. Our findings reveals that even with the LLM parameters\nfixed, the model could effectively model pixel dependence and generate reasonable images\nbased on category names.\n‚Ä¢ Focused Training in Stage II: In Stage II, we drop ImageNet data and directly utilize nor-\nmal text-to-image data to train the model to generate images based on dense descriptions.\nThis redesigned approach enables Stage II to utilize the text-to-image data more efficiently,\nresulting in improved training efficiency and overall performance.\nWe also adjust the data ratio in Stage III supervised fine-tuning process across different types\nof datasets, changing the proportion of multimodal data, pure text data, and text-to-image data\nfrom 7:3:10 to 5:1:4. By slightly reducing the proportion of text-to-image data, we observe that\nthis adjustment allows us to maintain strong visual generation capabilities while achieving\nimproved multimodal understanding performance.\n2.3. Data Scaling\nWe scale up the training data used for Janus in both multimodal understanding and visual\ngeneration aspects.\n‚Ä¢ Multimodal Understanding. For the Stage II pretraining data, we refer to DeepSeek-\nVL2 [49] and add approximately 90 million samples. These include image caption datasets\n(e.g., YFCC [31]), as well as data for table, chart, and document understanding (e.g., Doc-\nmatix [20]). For the Stage III supervised fine-tuning data, we also incorporate additional\ndatasets from DeepSeek-VL2, such as MEME understanding, Chinese conversational data,\nand datasets aimed at enhancing dialogue experiences. These additions significantly\nexpanded the model‚Äôs capabilities, enriching its ability to handle diverse tasks while\nimproving the overall conversational experience.\n‚Ä¢ Visual Generation. We observe that the real-world data used in the previous version of\nJanus lacks quality and contains significant noise, which often leads to instability in text-\nto-image generation, resulting in aesthetically poor outputs. In Janus-Pro, we incorporate\napproximately 72 million samples of synthetic aesthetic data, bringing the ratio of real to\nsynthetic data to 1:1 during the unified pretraining stage. The prompts for these synthetic\ndata samples are publicly available, such as those in [43]. Experiments demonstrat that the\nmodel converges faster when trained on synthetic data, and the resulting text-to-image\noutputs are not only more stable but also exhibit significantly improved aesthetic quality.\n2.4. Model Scaling\nThe previous version of Janus validates the effectiveness of visual encoding decoupling using\na 1.5B LLM. In Janus-Pro, we scaled the model up to 7B, with the hyperparameters of both\nthe 1.5B and 7B LLMs detailed in Table 1. We observe that when utilizing a larger-scale LLM,\nthe convergence speed of losses for both multimodal understanding and visual generation\nimproved significantly compared to the smaller model. This finding further validates the strong\nscalability of this approach.\n4",
      "metadata": {
        "chunk_id": 4,
        "page_number": 4,
        "page_range": "4",
        "word_count": 478
      }
    },
    {
      "content": "Table 1 | Architectural configuration for Janus-Pro. We list the hyperparameters of the architec-\nture.\nJanus-Pro-1B\nJanus-Pro-7B\nVocabulary size\n100K\n100K\nEmbedding size\n2048\n4096\nContext Window\n4096\n4096\n#Attention heads\n16\n32\n#Layers\n24\n30\nTable 2 | Detailed hyperparameters for training Janus-Pro. Data ratio refers to the ratio of\nmultimodal understanding data, pure text data, and visual generation data.\nJanus-Pro-1B\nJanus-Pro-7B\nHyperparameters\nStage 1\nStage 2\nStage 3\nStage 1\nStage 2\nStage 3\nLearning rate\n1.0 √ó 10‚àí3 1.0 √ó 10‚àí4 4.0 √ó 10‚àí5 1.0 √ó 10‚àí3 1.0 √ó 10‚àí4 4.0 √ó 10‚àí5\nLR scheduler\nConstant Constant Constant Constant Constant Constant\nWeight decay\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nGradient clip\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nOptimizer\nAdamW (ùõΩ1 = 0.9, ùõΩ2 = 0.95)\nAdamW (ùõΩ1 = 0.9, ùõΩ2 = 0.95)\nWarm-up steps\n600\n5000\n0\n600\n5000\n0\nTraining steps\n20K\n360K\n80K\n20K\n360K\n40K\nBatch size\n256\n512\n128\n256\n512\n128\nData Ratio\n1:0:3\n2:3:5\n5:1:4\n1:0:3\n2:3:5\n5:1:4\n3. Experiments\n3.1. Implementation Details\nIn our experiments, we utilize DeepSeek-LLM (1.5B and 7B) [3] with a maximum supported\nsequence length of 4096 as the base language model. For the vision encoder used in understand-\ning tasks, we select SigLIP-Large-Patch16-384 [53]. The generation encoder has a codebook of\nsize 16, 384 and downsamples images by a factor of 16. Both the understanding adaptor and\nthe generation adaptor are two-layer MLPs. The detailed hyperparameters for each stage are\nprovided in Table 2. Please note that for Stage II, we employ an early stopping strategy, halting\nat 270K steps. All images are resized to 384 √ó 384 pixels. For multimodal understanding data,\nwe resize the long side of the image and pad the short side with the background color (RGB: 127,\n127, 127) to reach 384. For visual generation data, the short side is resized to 384, and the long\nside is cropped to 384. We use sequence packing during training to improve training efficiency.\nWe mix all data types according to the specified ratios in a single training step. Our Janus-Pro\nis trained and evaluated using HAI-LLM [15], which is a lightweight and efficient distributed\ntraining framework built on top of PyTorch. The whole training process took about 9/14 days\non a cluster of 16/32 nodes for 1.5B/7B model, each equipped with 8 Nvidia A100 (40GB) GPUs.\n3.2. Evaluation Setup\nMultimodal Understanding. To assess multimodal understanding capabilities, we evaluate our\nmodel on widely recognized image-based vision-language benchmarks, which include GQA\n5",
      "metadata": {
        "chunk_id": 5,
        "page_number": 5,
        "page_range": "5",
        "word_count": 417
      }
    },
    {
      "content": "Table 3 | Comparison with state-of-the-arts on multimodal understanding benchmarks. ‚ÄúUnd.‚Äù\nand ‚ÄúGen.‚Äù denote ‚Äúunderstanding‚Äù and ‚Äúgeneration‚Äù, respectively. Models using external\npretrained diffusion model are marked with ‚Ä†.\nType\nModel\n# LLM Params POPE‚ÜëMME-P‚ÜëMMB‚ÜëSEED‚ÜëGQA‚ÜëMMMU‚ÜëMM-Vet‚Üë\nUnd. Only\nLLaVA-v1.5-Phi-1.5 [50]\n1.3B\n84.1\n1128.0\n-\n-\n56.5\n30.7\n-\nMobileVLM [6]\n1.4B\n84.5\n1196.2\n53.2\n-\n56.1\n-\n-\nMobileVLM-V2 [7]\n1.4B\n84.3\n1302.8\n57.7\n-\n59.3\n-\n-\nMobileVLM [6]\n2.7B\n84.9\n1288.9\n59.6\n-\n59.0\n-\n-\nMobileVLM-V2 [7]\n2.7B\n84.7\n1440.5\n63.2\n-\n61.1\n-\n-\nLLaVA-Phi [56]\n2.7B\n85.0\n1335.1\n59.8\n-\n-\n-\n28.9\nLLaVA [27]\n7B\n76.3\n809.6\n38.7\n33.5\n-\n-\n25.5\nLLaVA-v1.5 [26]\n7B\n85.9\n1510.7\n64.3\n58.6\n62.0\n35.4\n31.1\nInstructBLIP [8]\n7B\n-\n-\n36.0\n53.4\n49.2\n-\n26.2\nQwen-VL-Chat [1]\n7B\n-\n1487.5\n60.6\n58.2\n57.5\n-\n-\nIDEFICS-9B [19]\n8B\n-\n-\n48.2\n-\n38.4\n-\n-\nEmu3-Chat [45]\n8B\n85.2\n1244\n58.5\n68.2\n60.3\n31.6\n37.2\nInstructBLIP [8]\n13B\n78.9\n1212.8\n-\n-\n49.5\n-\n25.6\nUnd. and Gen. DreamLLM‚Ä† [10]\n7B\n-\n-\n-\n-\n-\n-\n36.6\nLaVIT‚Ä† [18]\n7B\n-\n-\n-\n-\n46.8\n-\n-\nMetaMorph‚Ä† [42]\n8B\n-\n-\n75.2\n71.8\n-\n-\n-\nEmu‚Ä† [39]\n13B\n-\n-\n-\n-\n-\n-\n-\nNExT-GPT‚Ä† [47]\n13B\n-\n-\n-\n-\n-\n-\n-\nShow-o-256 [50]\n1.3B\n73.8\n948.4\n-\n-\n48.7\n25.1\n-\nShow-o-512 [50]\n1.3B\n80.0\n1097.2\n-\n-\n58.0\n26.7\n-\nD-Dit [24]\n2.0B\n84.0\n1124.7\n-\n-\n59.2\n-\n-\nGemini-Nano-1 [41]\n1.8B\n-\n-\n-\n-\n-\n26.3\n-\nILLUME [44]\n7B\n88.5\n1445.3\n65.1\n72.9\n‚àí\n38.2\n37.0\nTokenFlow-XL [34]\n13B\n86.8\n1545.9\n68.9\n68.7\n62.7\n38.7\n40.7\nLWM [28]\n7B\n75.2\n-\n-\n-\n44.8\n-\n9.6\nVILA-U [48]\n7B\n85.8\n1401.8\n-\n59.0\n60.8\n-\n33.5\nChameleon [40]\n7B\n-\n-\n-\n-\n-\n22.4\n8.3\nJanus\n1.5B\n87.0\n1338.0\n69.4\n63.7\n59.1\n30.5\n34.3\nJanus-Pro-1B\n1.5B\n86.2\n1444.0\n75.5\n68.3\n59.3\n36.3\n39.8\nJanus-Pro-7B\n7B\n87.4\n1567.1\n79.2\n72.1\n62.0\n41.0\n50.0\n[17], POPE [23], MME [12], SEED [21], MMB [29], MM-Vet [51], and MMMU [52].\nVisual Generation. For evaluating visual generation capabilities, we use GenEval [14] and\nDPG-Bench [16]. GenEval is a challenging benchmark for text-to-image generation, designed to\nreflect the comprehensive generative abilities of visual generation models by offering a detailed\ninstance-level analysis of their compositional capabilities. DPG-Bench (Dense Prompt Graph\nBenchmark) is a comprehensive dataset consisting of 1065 lengthy, dense prompts, designed to\nassess the intricate semantic alignment capabilities of text-to-image models.\n3.3. Comparison with State-of-the-arts\nMultimodal Understanding Performance. We compare the proposed method with state-of-\nthe-art unified models and understanding-only models in Table 3. Janus-Pro achieves the\noverall best results. This can be attributed to decoupling the visual encoding for multimodal\nunderstanding and generation, mitigating the conflict between these two tasks. When compared\nto models with significantly larger sizes, Janus-Pro remains highly competitive. For instance,\nJanus-Pro-7B outperforms TokenFlow-XL (13B) on all benchmarks except GQA.\n6",
      "metadata": {
        "chunk_id": 6,
        "page_number": 6,
        "page_range": "6",
        "word_count": 493
      }
    },
    {
      "content": "Table 4 | Evaluation of text-to-image generation ability on GenEval benchmark. ‚ÄúUnd.‚Äù\nand ‚ÄúGen.‚Äù denote ‚Äúunderstanding‚Äù and ‚Äúgeneration‚Äù, respectively. Models using external\npretrained diffusion model are marked with ‚Ä†.\nType\nMethod\nSingle Obj.\nTwo Obj.\nCounting\nColors\nPosition\nColor Attri.\nOverall‚Üë\nGen. Only\nLlamaGen [38]\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n0.32\nLDM [37]\n0.92\n0.29\n0.23\n0.70\n0.02\n0.05\n0.37\nSDv1.5 [37]\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\nPixArt-ùõº[4]\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\nSDv2.1 [37]\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n0.50\nDALL-E 2 [35]\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\nEmu3-Gen [45]\n0.98\n0.71\n0.34\n0.81\n0.17\n0.21\n0.54\nSDXL [32]\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\nDALL-E 3 [2]\n0.96\n0.87\n0.47\n0.83\n0.43\n0.45\n0.67\nSD3-Medium [11]\n0.99\n0.94\n0.72\n0.89\n0.33\n0.60\n0.74\nUnd. and Gen.\nSEED-X‚Ä† [13]\n0.97\n0.58\n0.26\n0.80\n0.19\n0.14\n0.49\nShow-o [50]\n0.95\n0.52\n0.49\n0.82\n0.11\n0.28\n0.53\nD-DiT [24]\n0.97\n0.80\n0.54\n0.76\n0.32\n0.50\n0.65\nLWM [28]\n0.93\n0.41\n0.46\n0.79\n0.09\n0.15\n0.47\nTransfusion [55]\n-\n-\n-\n-\n-\n-\n0.63\nILLUME [44]\n0.99\n0.86\n0.45\n0.71\n0.39\n0.28\n0.61\nTokenFlow-XL [28]\n0.95\n0.60\n0.41\n0.81\n0.16\n0.24\n0.55\nChameleon [40]\n-\n-\n-\n-\n-\n-\n0.39\nJanus [46]\n0.97\n0.68\n0.30\n0.84\n0.46\n0.42\n0.61\nJanus-Pro-1B\n0.98\n0.82\n0.51\n0.89\n0.65\n0.56\n0.73\nJanus-Pro-7B\n0.99\n0.89\n0.59\n0.90\n0.79\n0.66\n0.80\nTable 5 | Performances on DPG-Bench. The methods in this table are all generation-specific\nmodels except Janus and Janus-Pro.\nMethod\nGlobal\nEntity\nAttribute\nRelation\nOther\nOverall‚Üë\nSDv1.5 [36]\n74.63\n74.23\n75.39\n73.49\n67.81\n63.18\nPixArt-ùõº[4]\n74.97\n79.32\n78.60\n82.57\n76.96\n71.11\nLumina-Next [57]\n82.82\n88.65\n86.44\n80.53\n81.82\n74.63\nSDXL [33]\n83.27\n82.43\n80.91\n86.76\n80.41\n74.65\nPlayground v2.5 [22]\n83.06\n82.59\n81.20\n84.08\n83.50\n75.47\nHunyuan-DiT [25]\n84.59\n80.59\n88.01\n74.36\n86.41\n78.87\nPixArt-Œ£ [5]\n86.89\n82.89\n88.94\n86.59\n87.68\n80.54\nEmu3-Gen [45]\n85.21\n86.68\n86.84\n90.22\n83.15\n80.60\nDALL-E 3 [2]\n90.97\n89.61\n88.39\n90.58\n89.83\n83.50\nSD3-Medium [11]\n87.90\n91.01\n88.83\n80.70\n88.68\n84.08\nJanus\n82.33\n87.38\n87.70\n85.46\n86.41\n79.68\nJanus-Pro-1B\n87.58\n88.63\n88.17\n88.98\n88.30\n82.63\nJanus-Pro-7B\n86.90\n88.90\n89.40\n89.32\n89.48\n84.19\nVisual Generation Performance. We report visual generation performance on GenEval and\nDPG-Bench. As shown in Table 4, our Janus-Pro-7B obtains 80% overall accuracy on GenEval,\nwhich outperforms all the other unified or generation-only methods, e.g., Transfusion [55] (63%)\nSD3-Medium (74%) and DALL-E 3 (67%). This demonstrates that our approach has better\ninstruction-following capabilities. As shown in Table 5, Janus-Pro achieves a score of 84.19 on\nDPG-Bench, surpassing all other methods. This demonstrates that Janus-Pro excels in following\ndense instructions for text-to-image generation.\n3.4. Qualitative Results\nWe present results on multimodal understanding in Figure 4. Janus-Pro exhibits impressive\ncomprehension abilities when handling inputs from various contexts, showcasing its powerful\ncapabilities. We also present some text-to-image generation results in the lower part of Figure 4.\nThe images generated by Janus-Pro-7B are highly realistic, and despite having a resolution of\nonly 384 √ó 384, they still contain a lot of details. For imaginative and creative scenes, Janus-Pro-\n7B accurately captures the semantic information from the prompts, producing well-reasoned\nand coherent images.\n7",
      "metadata": {
        "chunk_id": 7,
        "page_number": 7,
        "page_range": "7",
        "word_count": 531
      }
    },
    {
      "content": "Text-To-Image Generation\nImage Description\nDescribe this scene in detail.\nThis image depicts a stunning coastal landscape featuring a series of tall, rugged rock formations emerging from the ocean.\nThe rocks are layered with sedimentary rock, giving them a distinct stratified appearance. The largest formation is\nprominently positioned in the foreground, with smaller ones receding into the distance. The ocean waves crash against the\nbase of these formations, creating white foam and a dynamic sense of movement.\nThe coastline is sandy, with a narrow strip of beach visible on the right side of the image. The beach is bordered by steep\ncliffs, which rise dramatically from the shoreline. The cliffs are covered in patches of green vegetation, adding a touch of\ncolor to the otherwise earthy tones of the rocks.\nThe sky above is overcast, with thick clouds that suggest an impending storm or recent rainfall. The light is diffused, casting a\nsoft, muted glow over the entire scene. Despite the cloudy sky, there is a sense of tranquility and natural beauty in this\ncoastal landscape.\nËøôÂº†Âõæ‚ΩöÂ±ïÁ§∫ÁöÑÊòØ‰∏≠ÂõΩÊù≠Â∑ûÁöÑ‚ªÑÊπñÔºåÁâπÂà´ÊòØËëóÂêçÁöÑ\n‰∏âÊΩ≠Âç∞‚ΩâÂ≤õ„ÄÇÂõæ‚Ωö‰∏≠ÁöÑÂª∫Á≠ëÊòØ‰∏âÊΩ≠Âç∞‚ΩâÁöÑ‰∫≠‚º¶ÔºåÂë®\nÂõ¥ÊòØÂπ≥ÈùôÁöÑÊπñ‚ΩîÔºåËøúÂ§ÑÊòØËøûÁªµÁöÑ‚º≠ËÑâ„ÄÇ‚ªÑÊπñÊòØ‰∏≠ÂõΩ\nËëóÂêçÁöÑÊóÖÊ∏∏ÊôØÁÇπ‰πã‚ºÄÔºå‰ª•ÂÖ∂Áæé‰∏ΩÁöÑ‚æÉÁÑ∂ÊôØËßÇÂíå‰∏∞ÂØå\nÁöÑÂéÜÂè≤‚ΩÇÂåñ‚ΩΩÈóªÂêç„ÄÇ\nÁåúÁåúËøôÊòØÂì™ÈáåÔºü\nLandmark Recognition\nWhat‚Äòs written on the large black board?\nThe large black board has the \nfollowing text:\n\"Serving Soul since Twenty Twelve\"\nAdditionally, there are smaller \nsections with more detailed \ninformation, but the main heading \nis clearly visible.\nText Recognition\nThis cake is themed around the classic cartoon \ncharacter Tom and Jerry. The cake features the \ncharacters Tom (the cat) and Jerry (the mouse) in a \nplayful setting, with a backdrop that includes the \nwords \"Tom & Jerry‚Äù. The cake itself is decorated \nwith fondant shaped like cheese, which is a \nrecurring theme in the cartoon series. The design \nalso includes a small fondant house with a door, \nadding to the whimsical and playful theme of the \ncake.\nCan you introduce the \nbackground story of this cake?\nGeneral Knowledge\nA young woman with freckles \nwearing a straw hat, standing \nin a golden wheat field.\nA single drop of water clinging \nto a green leaf, with sunlight \ncreating a faint rainbow pris\nAn ancient stone bridge \narching over a crystal-clear \nmountain stream, surrounded \nby lush greenery.\nA golden retriever lying peacefully \non a wooden porch, with autumn \nleaves scattered around.\nA glowing crystal ball floating \nabove a sandstone table in the \nmiddle of a desert at sunset.\nA tiny galaxy contained inside \na glass bottle, glowing brightly \nagainst a dark velvet cloth.\nA giant whale flying through a \ncity skyline, surrounded by \nfloating glowing lanterns.\nAstronaut in a jungle, cold \ncolor palette, muted colors, \ndetailed, 8k\nFigure 4 | Qualitative results of multimodal understanding and visual generation capability.\nThe model is Janus-Pro-7B and the image output resolution of visual generation is 384 √ó 384.\nBest viewed on screen.\n8",
      "metadata": {
        "chunk_id": 8,
        "page_number": 8,
        "page_range": "8",
        "word_count": 459
      }
    },
    {
      "content": "4. Conclusion\nThis paper introduces improvements to Janus from three aspects: training strategy, data, and\nmodel size. These enhancements have led to significant advancements in both multimodal\nunderstanding and text-to-image instruction-following capabilities. However, Janus-Pro still\nhas certain limitations. In terms of multimodal understanding, the input resolution is limited\nto 384 √ó 384, which affects its performance in fine-grained tasks such as OCR. For text-to-\nimage generation, the low resolution, combined with reconstruction losses introduced by the\nvision tokenizer, results in images that, while rich in semantic content, still lack fine details.\nFor example, small facial regions occupying limited image space may appear under-detailed.\nIncreasing the image resolution could mitigate these issues.\nReferences\n[1] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A fron-\ntier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966,\n2023.\n[2] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo,\net al. Improving image generation with better captions. Computer Science. https://cdn.\nopenai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\n[3] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al.\nDeepseek llm: Scaling open-source language models with longtermism. arXiv preprint\narXiv:2401.02954, 2024.\n[4] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, et al. Pixart-\nùëéùëôùëù‚Ñéùëé: Fast training of diffusion transformer for photorealistic text-to-image synthesis.\narXiv preprint arXiv:2310.00426, 2023.\n[5] J. Chen, C. Ge, E. Xie, Y. Wu, L. Yao, X. Ren, Z. Wang, P. Luo, H. Lu, and Z. Li. PixArt-Sigma:\nWeak-to-strong training of diffusion transformer for 4K text-to-image generation. arXiv\npreprint arXiv:2403.04692, 2024.\n[6] X. Chu, L. Qiao, X. Lin, S. Xu, Y. Yang, Y. Hu, F. Wei, X. Zhang, B. Zhang, X. Wei, et al.\nMobilevlm: A fast, reproducible and strong vision language assistant for mobile devices.\narXiv preprint arXiv:2312.16886, 2023.\n[7] X. Chu, L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang, et al.\nMobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint\narXiv:2402.03766, 2024.\n[8] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip:\nTowards general-purpose vision-language models with instruction tuning, 2023.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchi-\ncal image database. In 2009 IEEE conference on computer vision and pattern recognition,\npages 248‚Äì255. Ieee, 2009.\n[10] R. Dong, C. Han, Y. Peng, Z. Qi, Z. Ge, J. Yang, L. Zhao, J. Sun, H. Zhou, H. Wei, et al. Dream-\nllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499,\n2023.\n9",
      "metadata": {
        "chunk_id": 9,
        "page_number": 9,
        "page_range": "9",
        "word_count": 469
      }
    },
    {
      "content": "[11] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. M√ºller, H. Saini, Y. Levi, D. Lorenz, A. Sauer,\nF. Boesel, D. Podell, T. Dockhorn, Z. English, K. Lacey, A. Goodwin, Y. Marek, and R. Rom-\nbach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL\nhttps://arxiv.org/abs/2403.03206.\n[12] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, et al.\nMme: A comprehensive evaluation benchmark for multimodal large language models.\narXiv preprint arXiv:2306.13394, 2023.\n[13] Y. Ge, S. Zhao, J. Zhu, Y. Ge, K. Yi, L. Song, C. Li, X. Ding, and Y. Shan. Seed-x: Multimodal\nmodels with unified multi-granularity comprehension and generation. arXiv preprint\narXiv:2404.14396, 2024.\n[14] D. Ghosh, H. Hajishirzi, and L. Schmidt. Geneval: An object-focused framework for\nevaluating text-to-image alignment. Advances in Neural Information Processing Systems,\n36, 2024.\n[15] High-flyer. Hai-llm: Efficient and lightweight training tool for large models, 2023. URL\nhttps://www.high-flyer.cn/en/blog/hai-llm.\n[16] X. Hu, R. Wang, Y. Fang, B. Fu, P. Cheng, and G. Yu. Ella: Equip diffusion models with llm\nfor enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024.\n[17] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 6700‚Äì6709, 2019.\n[18] Y. Jin, K. Xu, L. Chen, C. Liao, J. Tan, B. Chen, C. Lei, A. Liu, C. Song, X. Lei, et al. Unified\nlanguage-vision pretraining with dynamic discrete visual tokenization. arXiv preprint\narXiv:2309.04669, 2023.\n[19] H. Lauren√ßon, D. van Strien, S. Bekman, L. Tronchon, L. Saulnier, T. Wang, S. Karamcheti,\nA. Singh, G. Pistilli, Y. Jernite, and et al. Introducing idefics: An open reproduction of\nstate-of-the-art visual language model, 2023. URL https://huggingface.co/blog/id\nefics.\n[20] H. Lauren√ßon, A. Marafioti, V. Sanh, and L. Tronchon. Building and better understanding\nvision-language models: insights and future directions., 2024.\n[21] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal\nllms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[22] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi. Playground v2.5: Three\ninsights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint\narXiv:2402.17245, 2024.\n[23] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in\nlarge vision-language models. arXiv preprint arXiv:2305.10355, 2023.\n[24] Z. Li, H. Li, Y. Shi, A. B. Farimani, Y. Kluger, L. Yang, and P. Wang. Dual diffusion for\nunified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024.\n[25] Z. Li, J. Zhang, Q. Lin, J. Xiong, Y. Long, X. Deng, Y. Zhang, X. Liu, M. Huang, Z. Xiao,\net al. Hunyuan-DiT: A powerful multi-resolution diffusion transformer with fine-grained\nchinese understanding. arXiv preprint arXiv:2405.08748, 2024.\n10",
      "metadata": {
        "chunk_id": 10,
        "page_number": 10,
        "page_range": "10",
        "word_count": 461
      }
    },
    {
      "content": "[26] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 26296‚Äì26306, 2024.\n[27] H. Liu, C. Li, Q. Wu, and Y. J. Lee.\nVisual instruction tuning.\nAdvances in neural\ninformation processing systems, 36, 2024.\n[28] H. Liu, W. Yan, M. Zaharia, and P. Abbeel. World model on million-length video and\nlanguage with ringattention. arXiv preprint arXiv:2402.08268, 2024.\n[29] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mm-\nbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281,\n2023.\n[30] Y. Ma, X. Liu, X. Chen, W. Liu, C. Wu, Z. Wu, Z. Pan, Z. Xie, H. Zhang, X. yu, L. Zhao,\nY. Wang, J. Liu, and C. Ruan. Janusflow: Harmonizing autoregression and rectified flow for\nunified multimodal understanding and generation, 2024.\n[31] mehdidc. Yfcc-huggingface. https://huggingface.co/datasets/mehdidc/yfcc15\nm, 2024.\n[32] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. M√ºller, J. Penna, and R. Rom-\nbach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952, 2023.\n[33] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. M√ºller, J. Penna, and R. Rom-\nbach. SDXL: Improving latent diffusion models for high-resolution image synthesis. 2024.\n[34] L. Qu, H. Zhang, Y. Liu, X. Wang, Y. Jiang, Y. Gao, H. Ye, D. K. Du, Z. Yuan, and X. Wu.\nTokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv\npreprint arXiv:2412.03069, 2024.\n[35] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\n[36] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image\nsynthesis with latent diffusion models. 2022.\n[37] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 10684‚Äì10695, 2022.\n[38] P. Sun, Y. Jiang, S. Chen, S. Zhang, B. Peng, P. Luo, and Z. Yuan. Autoregressive model beats\ndiffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024.\n[39] Q. Sun, Q. Yu, Y. Cui, F. Zhang, X. Zhang, Y. Wang, H. Gao, J. Liu, T. Huang, and X. Wang.\nGenerative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023.\n[40] C. Team. Chameleon: Mixed-modal early-fusion foundation models.\narXiv preprint\narXiv:2405.09818, 2024.\n[41] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.\nDai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023.\n11",
      "metadata": {
        "chunk_id": 11,
        "page_number": 11,
        "page_range": "11",
        "word_count": 445
      }
    },
    {
      "content": "[42] S. Tong, D. Fan, J. Zhu, Y. Xiong, X. Chen, K. Sinha, M. Rabbat, Y. LeCun, S. Xie, and\nZ. Liu. Metamorph: Multimodal understanding and generation via instruction tuning.\narXiv preprint arXiv:2412.14164, 2024.\n[43] Vivym. Midjourney prompts dataset. https://huggingface.co/datasets/vivym/\nmidjourney-prompts, 2023. Accessed: [Insert Date of Access, e.g., 2023-10-15].\n[44] C. Wang, G. Lu, J. Yang, R. Huang, J. Han, L. Hou, W. Zhang, and H. Xu. Illume: Il-\nluminating your llms to see, draw, and self-enhance. arXiv preprint arXiv:2412.06673,\n2024.\n[45] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu, et al.\nEmu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024.\n[46] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan, et al. Janus:\nDecoupling visual encoding for unified multimodal understanding and generation. arXiv\npreprint arXiv:2410.13848, 2024.\n[47] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua. Next-gpt: Any-to-any multimodal llm. arXiv\npreprint arXiv:2309.05519, 2023.\n[48] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi, et al. Vila-u: a\nunified foundation model integrating visual understanding and generation. arXiv preprint\narXiv:2409.04429, 2024.\n[49] Z. Wu, X. Chen, Z. Pan, X. Liu, W. Liu, D. Dai, H. Gao, Y. Ma, C. Wu, B. Wang, et al.\nDeepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal un-\nderstanding. arXiv preprint arXiv:2412.10302, 2024.\n[50] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and M. Z.\nShou. Show-o: One single transformer to unify multimodal understanding and generation.\narXiv preprint arXiv:2408.12528, 2024.\n[51] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating\nlarge multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\n[52] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al.\nMmmu: A massive multi-discipline multimodal understanding and reasoning benchmark\nfor expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9556‚Äì9567, 2024.\n[53] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-\ntraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 11975‚Äì11986, 2023.\n[54] C. Zhao, Y. Song, W. Wang, H. Feng, E. Ding, Y. Sun, X. Xiao, and J. Wang. Monoformer:\nOne transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280,\n2024.\n[55] C. Zhou, L. Yu, A. Babu, K. Tirumala, M. Yasunaga, L. Shamis, J. Kahn, X. Ma, L. Zettle-\nmoyer, and O. Levy. Transfusion: Predict the next token and diffuse images with one\nmulti-modal model. arXiv preprint arXiv:2408.11039, 2024.\n[56] Y. Zhu, M. Zhu, N. Liu, Z. Ou, X. Mou, and J. Tang. Llava-phi: Efficient multi-modal\nassistant with small language model. arXiv preprint arXiv:2401.02330, 2024.\n12",
      "metadata": {
        "chunk_id": 12,
        "page_number": 12,
        "page_range": "12",
        "word_count": 490
      }
    },
    {
      "content": "[57] L. Zhuo, R. Du, H. Xiao, Y. Li, D. Liu, R. Huang, W. Liu, L. Zhao, F.-Y. Wang, Z. Ma, et al.\nLumina-Next: Making Lumina-T2X stronger and faster with Next-DiT. arXiv preprint\narXiv:2406.18583, 2024.\n13",
      "metadata": {
        "chunk_id": 13,
        "page_number": 13,
        "page_range": "13",
        "word_count": 36
      }
    }
  ]
}